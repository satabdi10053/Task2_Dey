from pyspark.sql.functions import current_timestamp
bronze_path = "abfss://bronze@<storage>.dfs.core.windows.net/member" schema_loc = "abfss://silver@<storage>.dfs.core.windows.net/_schemas/gym/member" df_bronze_stream = ( spark.readStream .format("cloudFiles") .option("cloudFiles.format", "json") # or csv/parquet .option("cloudFiles.schemaLocation", schema_loc) .load("abfss://raw@<storage>.dfs.core.windows.net/gym/member_changes") .withColumn("ingestion_ts", current_timestamp()) ) ( df_bronze_stream.writeStream .format("delta") .option("checkpointLocation", f"{bronze_path}/_checkpoint") .outputMode("append") .start(bronze_path) )
